{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task\n",
    "Our task is simple, recognize handwritten digits. We will use MNIST dataset for this tutorial.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import necessary library\n",
    "In this tutorial, we are going to use pytorch, the cutting-edge deep learning framework to complete our task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create dataloader, in PyTorch, we feed the trainer data with use of dataloader\n",
    "## We create dataloader with dataset from torchvision, \n",
    "## and we dont have to download it seperately, all automatically done\n",
    "\n",
    "# Define batch size, batch size is how much data you feed for training in one iteration\n",
    "batch_size_train = 64 # We use a small batch size here for training\n",
    "batch_size_test = 1024 #\n",
    "\n",
    "# define how image transformed\n",
    "image_transform = torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor(),\n",
    "                               torchvision.transforms.Normalize(\n",
    "                                 (0.1307,), (0.3081,))\n",
    "                             ])\n",
    "#image datasets\n",
    "train_dataset = torchvision.datasets.MNIST('dataset/', \n",
    "                                           train=True, \n",
    "                                           download=True,\n",
    "                                           transform=image_transform)\n",
    "test_dataset = torchvision.datasets.MNIST('dataset/', \n",
    "                                          train=False, \n",
    "                                          download=True,\n",
    "                                          transform=image_transform)\n",
    "#data loaders\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset,\n",
    "                                           batch_size=batch_size_train, \n",
    "                                           shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset,\n",
    "                                          batch_size=batch_size_test, \n",
    "                                          shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: tensor(6)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# import library\n",
    "import matplotlib.pyplot as plt\n",
    "# We can check the dataloader\n",
    "_, (example_datas, labels) = next(enumerate(test_loader))\n",
    "sample = example_datas[0][0]\n",
    "# show the data\n",
    "plt.imshow(sample, cmap='gray', interpolation='none')\n",
    "print(\"Label: \"+ str(labels[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Now we can start to build our CNN model\n",
    "## We first import the pytorch nn module and optimizer\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "## Then define the model class\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        #input channel 1, output channel 10\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5, stride=1)\n",
    "        #input channel 10, output channel 20\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5, stride=1)\n",
    "        #dropout layer\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        #fully connected layer\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv2_drop(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = F.relu(x)\n",
    "        x = x.view(-1, 320)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create model and optimizer\n",
    "learning_rate = 0.01\n",
    "momentum = 0.5\n",
    "device = \"cuda\"\n",
    "model = CNN().to(device) #using gpu here\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate,\n",
    "                      momentum=momentum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook as tqdm\n",
    "##define train function\n",
    "def train(model, device, train_loader, optimizer, epoch, log_interval=10000):\n",
    "    model.train()\n",
    "    tk0 = tqdm(train_loader, total=int(len(train_loader)))\n",
    "    counter = 0\n",
    "    for batch_idx, (data, target) in enumerate(tk0):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        counter += 1\n",
    "        tk0.set_postfix(loss=(loss.item()*data.size(0) / (counter * train_loader.batch_size)))\n",
    "##define test function\n",
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item() # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True) # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Haqqi\\AppData\\Local\\Temp\\ipykernel_19452\\1895196525.py:5: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  tk0 = tqdm(train_loader, total=int(len(train_loader)))\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.004991292953491211,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 938,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "300073f22dc548d1b9d8fd6bfa45b116",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/938 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Haqqi\\AppData\\Local\\Temp\\ipykernel_19452\\4020281764.py:32: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.log_softmax(x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.3075, Accuracy: 9091/10000 (91%)\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.004003047943115234,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 938,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b9e7d5d268d45fa8b5b0d00a93ec2c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/938 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.2125, Accuracy: 9373/10000 (94%)\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.004988908767700195,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 938,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "051091ff115b4341b3a5c384172c61ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/938 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.1764, Accuracy: 9473/10000 (95%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_epoch = 3\n",
    "for epoch in range(1, num_epoch + 1):\n",
    "        train(model, device, train_loader, optimizer, epoch)\n",
    "        test(model, device, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchsummary in c:\\users\\haqqi\\miniconda3\\envs\\jcopdl\\lib\\site-packages (1.5.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torchsummary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 10, 24, 24]             260\n",
      "            Conv2d-2             [-1, 20, 8, 8]           5,020\n",
      "         Dropout2d-3             [-1, 20, 8, 8]               0\n",
      "            Linear-4                   [-1, 50]          16,050\n",
      "            Linear-5                   [-1, 10]             510\n",
      "================================================================\n",
      "Total params: 21,840\n",
      "Trainable params: 21,840\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.06\n",
      "Params size (MB): 0.08\n",
      "Estimated Total Size (MB): 0.15\n",
      "----------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Haqqi\\AppData\\Local\\Temp\\ipykernel_19452\\4020281764.py:32: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.log_softmax(x)\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "summary(model, (1, 28, 28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
